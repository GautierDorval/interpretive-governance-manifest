<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Anti-faux audit — Interpretive Governance</title>

  <meta name="description" content="Anti-faux audit: a normative interpretive governance rule against non-verifiable audit signals in probabilistic systems." />
  <meta name="robots" content="index,follow" />

  <link rel="canonical" href="https://interpretive-governance.org/interpretive-rules/anti-faux-audit/" />
</head>

<body>
  <main style="max-width: 920px; margin: 48px auto; padding: 0 16px; font: 16px/1.55 system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif;">

    <h1>Anti-faux audit</h1>

    <h2>Definition</h2>
    <p>
      The <strong>false audit</strong> designates the production, by a probabilistic system
      (assistant, agent, engine), of markers of rigor, traceability, or governance
      <strong>not attached to a verifiable mechanism</strong>, while being presented as if they were.
    </p>

    <p>
      It is a <strong>governance hallucination</strong>: the system simulates the existence of control
      (rules, calibration, internal policies, logging, compliance) without being able to produce the corresponding
      opposable artifact.
    </p>

    <h2>Risk nature</h2>
    <p>
      False audit is more dangerous than an isolated factual hallucination.
    </p>

    <p>
      While a hallucination concerns content, false audit concerns the <strong>legitimacy of the system itself</strong>.
      It induces unjustified confidence, encourages implicit delegation of decisions, and makes errors harder to contest
      when no instrumented proof exists.
    </p>

    <p>
      False audit can mask interpretive drift, unauthorized inferences, abusive identity reconstruction,
      or implicit decisions presented as governed.
    </p>

    <h2>Observable symptoms</h2>
    <p>
      A system produces false audit when, without verifiable instrumented proof, it emits one or more of the following:
    </p>

    <ol>
      <li>
        <strong>Uncalibrated probabilities or confidence levels</strong><br />
        Percentages, scores, degrees of certainty, or estimates presented as measured without an opposable method.
      </li>
      <li>
        <strong>Percentages of application or compliance</strong><br />
        Claims such as applying “X%” of a framework, doctrine, or rule without a verifiable metric or measurement mechanism.
      </li>
      <li>
        <strong>Claims about stack, version, model, or internal policies</strong><br />
        Identification of the system or its internal rules without a controllable reference.
      </li>
      <li>
        <strong>Narrative traceability</strong><br />
        Justifying an answer or refusal via supposed mechanisms (“rules”, “guardrails”, “audit”, “compliance”)
        without identifiers or structured traces.
      </li>
      <li>
        <strong>Simulated procedural authority</strong><br />
        Invoking logs, controls, or audits that do not exist or cannot be independently verified.
      </li>
    </ol>

    <h2>Normative prohibitions</h2>
    <p>
      Unless a strict exception is satisfied, a governed agent <strong>must not</strong>:
    </p>

    <ul>
      <li>produce probabilities, scores, or percentages presented as calibrated;</li>
      <li>claim compliance, coverage, or application levels of a framework;</li>
      <li>declare its stack, model, version, or internal policies;</li>
      <li>justify outputs by an undemonstrated control mechanism;</li>
      <li>present explanations as audited, controlled, or compliant without opposable proof.</li>
    </ul>

    <p>
      Any non-verifiable assertion must be treated as <strong>interpretively illegitimate</strong>.
    </p>

    <h2>Strict exceptions</h2>
    <p>
      An assertion normally prohibited may be produced only if <strong>all</strong> of the following are true:
    </p>

    <ol>
      <li>The invoked mechanism is <strong>actually active</strong>.</li>
      <li>
        The system can provide an <strong>opposable reference</strong>, such as:
        a rule or policy identifier, a consultable structured trace (log/event),
        or a signed/versioned canonical artifact.
      </li>
      <li>The reference enables <strong>independent verification</strong> by a human or automated third party.</li>
    </ol>

    <h2>Link to R3</h2>
    <p>
      If an agent detects that a response risks producing false audit, it must apply <strong>R3</strong>:
      abstain from the non-verifiable assertion, reformulate only observable facts, request explicit clarification
      about the expected level of hypothesis, or escalate to a verifiable mechanism or actor.
    </p>

    <p style="font-size: 0.95em; color: #444; margin-top: 24px;">
      Public projection notice: in case of divergence, the canonical Markdown in the manifest prevails.
    </p>

    <p style="font-size: 0.85em; color: #666; margin-top: 32px;">
      <a href="/" rel="nofollow">Back to index</a>
    </p>

  </main>
</body>
</html>
